<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>TTCME.pdf API documentation</title>
<meta name="description" content="This contains the basic probability density function `pdfTT` represented using tensor product basis and TT DoFs." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>TTCME.pdf</code></h1>
</header>
<section id="section-intro">
<p>This contains the basic probability density function <code><a title="TTCME.pdf.pdfTT" href="#TTCME.pdf.pdfTT">pdfTT</a></code> represented using tensor product basis and TT DoFs.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
This contains the basic probability density function `pdfTT` represented using tensor product basis and TT DoFs.

&#34;&#34;&#34;
import torchtt as tntt
import torch as tn
import torch
import numpy as np
from .basis import BSplineBasis, LagrangeBasis, LegendreBasis, ChebyBasis, DiracDeltaBase, UnivariateBasis
import copy
import TTCME

def GammaPDF(alphas, betas, basis, variable_names = []):
    &#34;&#34;&#34;
    Compute the PDF for multivariate distribution of independent gammas. 

    Args:
        alphas (numpy.array): the alphas of the distribution.
        betas (numpy.array): the betas of the distribution. 
        basis (list[TTCME.basis.UnivariateBasis]): list of the bases.
        variable_names (list[str], optional): the variable names. Defaults to [].

    Returns:
        pdfTT: the PDF instance.
    &#34;&#34;&#34;
    pdf = pdfTT(basis, variable_names = variable_names)
    # print(basis.interpolate(lambda x : x**(alpha-1) * np.exp(-beta*x)))
    tts = []
    for i in range(len(basis)):
        p, M = basis[i].interpolation_pts
        f = lambda x : x**(alphas[i]-1) * np.exp(-betas[i]*x)
        tts.append(tn.tensor(np.linalg.solve(M,f(p))))
    pdf.dofs = tntt.rank1TT(tts)
    pdf.normalize()        
    return pdf

def UniformPDF( basis, variable_names = []):
    &#34;&#34;&#34;
    Compute the PDF for multivariate uniform RVs. 

    Args:
        basis (list[TTCME.basis.UnivariateBasis]): list of the bases.
        variable_names (list[str], optional): the variable names. Defaults to [].

    Returns:
        pdfTT: the PDF instance.
    &#34;&#34;&#34;
    
    pdf = pdfTT(basis, variable_names = variable_names)
    # print(basis.interpolate(lambda x : x**(alpha-1) * np.exp(-beta*x)))
    tts = []
    for i in range(len(basis)):
        p, M = basis[i].interpolation_pts
        f = lambda x : x*0+1.0
        tts.append(tn.tensor(np.linalg.solve(M,f(p))))
    pdf.update(tntt.rank1TT(tts))
    pdf.normalize()        
    return pdf


def SingularPMF(N,I, variable_names = []):
    
    dofs =  tntt.rank1TT([tn.eye(N[i])[I[i],:] for i in range(len(N))])
    basis = [DiracDeltaBase(n) for n in N]

    return pdfTT(basis, [], variable_names, [], dofs)
    
def BetaPdfTT(N : list[int], alphas: list[float], betas: list[float]):
    &#34;&#34;&#34;
    Compute the PDF for independent beta distributed RVs. 

    Args:
        N (list[int]): the size of the univariate bases.
        alphas (list[float]): list of the values of alpha.
        betas (list[float]): list of values of beta.

    Returns:
        pdfTT: the pdf instance.
    &#34;&#34;&#34;
    
    basis = [BSplineBasis(n,[0,1],2) for n in N]
    vects = []
    pdf_g = lambda x,alpha,beta: x**(alpha-1)*(1-x)**(beta-1)
    for i in range(len(N)):
        p, M = basis[i].interpolation_pts
        vects.append(tn.tensor(np.linalg.solve(M,pdf_g(p,alphas[i],betas[i]))))

    pdf = pdfTT(basis,[],[],[],tntt.rank1TT(vects)) 
    pdf.normalize()
    return pdf

class LogNormalObservation:

    def __init__(self, N, sigmas):
        &#34;&#34;&#34;
        Implements the Gaussian observation operator class for the CME:

        $$ p(\\mathbf{y} | \\mathbf{x}) \sim \prod\limits_{k=1}^d \\frac{1}{y_k \sigma_k \sqrt{2 \pi}} \exp(-\\frac{1}{2}\\frac{(\log{y_k}-\log{(x_k+1)})^2}{\sigma_k^2})$$
        
        Args:
            N (list[int]): the state truncation.
            sigmas (list[float]): the variances of the independent Gaussians.
        &#34;&#34;&#34;
        self.__N = N
        self.__sigmas = sigmas
        
    def likelihood(self, observation):
        &#34;&#34;&#34;
        Computes the likelihood given an observation and returns it a a tensor in the TT format.
        The returned tensor is
        
        $$ \mathsf{p}^{(\\text{obs})}_{i_1...i_d} = p(\\mathbf{y} | \\mathbf{x} = (i_1,...,i_d) ),$$ 
        
        where \( \\mathbf{y} \) is the given observation.

        Args:
            observation (numpy.array): the observation vector.

        Returns:
            torchtt.TT: the likelihood.
        &#34;&#34;&#34;
        noise_model = lambda x,y,s : 1/(y*s*np.sqrt(2*np.pi)) * np.exp(-(np.log(y)-np.log(x+1))**2/(2*s**2))

        tens = tntt.rank1TT([tn.tensor(noise_model(np.arange(self.__N[i]),observation[i],self.__sigmas[i])) for i in range(len(self.__N))])

        return tens

    @staticmethod
    def add_noise(sample, sigmas):
        &#34;&#34;&#34;
        Adds noise to the given sample according to the pdf.

        Args:
            sample (np.array): m x d array containing the number of eact species at disrete time steps. d is the number of species and m is the number of observations.
            sigmas (list[float]): the sigmas of the normal random number generator.

        Returns:
            np.array: the resulting sample with noise.
        &#34;&#34;&#34;
        lst = [ np.random.lognormal(np.log(sample[:,i]+1),sigmas[i]).reshape([-1,1]) for i in range(len(sigmas)) ]
        
        sample = np.hstack(tuple(lst))
        
        return sample

class GaussianObservation:

    def __init__(self, N, sigmas):
        &#34;&#34;&#34;
        Implements the Gaussian observation operator class for the CME:

        $$ p(\mathbf{y} | \mathbf{x}) \sim \prod\limits_{k=1}^d \exp(-\\frac{1}{2}\\frac{(y_k-x_k)^2}{\sigma_k^2})$$
        
        Args:
            N (list[int]): the state truncation.
            sigmas (list[float]): the variances of the independent Gaussians.
        &#34;&#34;&#34;
        self.__N = N
        self.__sigmas = sigmas
        
    def likelihood(self, observation):
        &#34;&#34;&#34;
        Computes the likelihood given an observation and returns it a a tensor in the TT format.
        The returned tensor is
        
        $$ \mathsf{p}^{(\\text{obs})}_{i_1...i_d} = p(\\mathbf{y} | \\mathbf{x} = (i_1,...,i_d) ),$$ 
        
        where \( \\mathbf{y} \) is the given observation.

        Args:
            observation (numpy.array): the observation vector.

        Returns:
            torchtt.TT: the likelihood.
        &#34;&#34;&#34;
        noise_model = lambda x,y,s :  np.exp(-(y-x)**2/(2*s**2))

        tens = tntt.rank1TT([tn.tensor(noise_model(np.arange(self.__N[i]),observation[i],self.__sigmas[i])) for i in range(len(self.__N))])

        return tens

    @staticmethod
    def add_noise(sample, sigmas ):
        &#34;&#34;&#34;
        Adds noise to the given sample accordinf to the pdf.

        Args:
            sample (np.array): m x d array containing the number of eact species at disrete time steps. d is the number of species and m is the number of observations.
            sigmas (list[float]): the sigmas of the normal random number generator.

        Returns:
            np.array: the resulting sample with noise.
        &#34;&#34;&#34;
        sample += np.random.normal(scale = sigmas, size=sample.shape)
        return sample

class pdfTT():
    def __init__(self, basis, basis_conditioned = [], variable_names = [], conditioned_variable_names = [], dofs = None):
        &#34;&#34;&#34;
        Probability density function approximation using a tensor-product basis:   
        $$ p(x_1,...,x_d|y_1,...,y_{d&#39;})  = \\sum\\limits_{k_1,...,k_d,n_1,...,n_{d&#39;}} \\mathsf{d}_{k_1...k_dn_1...n_d} b^{(1)}(x_1) \\cdots b^{(d)}(x_d) b&#39;^{(1)}(y_1) \\cdots b&#39;^{(d&#39;)}(y_{d&#39;}),$$
        where \( \\mathsf{d}\) is the DoF tensor and is represented in the TT format.
        
        Using the subscript operator, the pdf can be evaluated at given values. Following possible ways of using the subscript operator are possible:
         
         - For a conditional PDF, the `Ellipsis` can be used to evaluate the variables we condition on. As an example, for the `pdfTT` instance `p` representing the conditional `p(a,b|c,d)`, the expression `p[...,1,2]` represents the PDF `p(a,b|c=1,d=2)`.
         - If only torcht.tensor of float instances are provided, the result is the tensor that results when the PDF is evaluated on the given tensor-product grid.


        Args:
            basis (list[UnivariateBasis]): the univariate bases used to construct the tensor-product basis (\( b^{(k)} \) in the formula above). 
            basis_conditioned (list[UnivariateBasis], optional): the univariate bases in case a conditional is used (corresponding to the variables right of the conditioned sign). In the equation above are denoted by \( b&#39;^{(k)} \). Defaults to [].
            variable_names (list[str], optional): the name of variables as a list of strings. Defaults to [].
            conditioned_variable_names (list[str], optional): the names of the bariables that we condition on. Defaults to [].
            dofs (torchtt.TT, optional): the dofs in the TT format. If None is provided an uniform PDF is created. Defaults to None.
        &#34;&#34;&#34;
        self.__d = len(basis)
        self.__dc = len(basis_conditioned) 
        self.__variable_names = variable_names
        self.__conditioned_variable_names = conditioned_variable_names
        self.__N = [b.dim for b in basis]
        self.__Nc = [b.dim for b in basis_conditioned]
        self.__basis = basis.copy()
        self.__basis_cond = basis_conditioned.copy()

        if dofs == None:
            self.__tt = tntt.ones(self.__N+self.__Nc)
        else:
            self.__tt = dofs.clone()


    def copy(self):
        &#34;&#34;&#34;
        Create a copy of the `pdfTT` instance.

        Returns:
            pdfTT: the copy.
        &#34;&#34;&#34;
        return copy.deepcopy(self)

    @property
    def basis(self):
        &#34;&#34;&#34;
        list[UnivariateBasis]: the basis used.
        &#34;&#34;&#34;
        return self.__basis.copy()

    @property
    def basis_conditioned(self):
        &#34;&#34;&#34;
        list[UnivariateBasis]: the basis used for the conditioned variables.
        &#34;&#34;&#34;
        return self.__basis_cond.copy()

    @property
    def variable_names(self):
        &#34;&#34;&#34;
        list[str]: the name of the variables.
        &#34;&#34;&#34;
        return self.__variable_names.copy()

    @property
    def conditioned_variable_names(self):
        &#34;&#34;&#34; 
        list[str]: the name of the variables we condition on.
        &#34;&#34;&#34;
        return self.__conditioned_variable_names.copy()

    @property
    def dofs(self):
        &#34;&#34;&#34;
        torchtt.TT: the dofs tensor in the TT format.
        &#34;&#34;&#34;
        return self.__tt

    @dofs.setter 
    def dofs(self, value):
        if not isinstance(value, tntt.TT):
            raise Exception(&#34;DOFS have to be in the TT format&#34;)

        if value.is_ttm or value.N != self.__tt.N:
            raise Exception(&#34;Dimensions must match&#34;)

        self.__tt = value
        
    @classmethod
    def interpoalte(cls, pdf, basis, basis_conditioned = [], variable_names = [], conditioned_variable_names = [], eps = 1e-10):
        &#34;&#34;&#34;
        Interpolate a pdf using the given basis.
        
        Example:
            ```
            import TTCME
            basis = [TTCME.basis.BSplineBasis(64,[0,1],2), TTCME.basis.BSplineBasis(64,[0,1],2)]
            basis_cond = [TTCME.basis.BSplineBasis(32,[2,3],2), TTCME.basis.BSplineBasis(43,[2, 3],2)]
            pdf_c = TTCME.pdf.pdfTT.interpoalte(pdf = lambda x: x[...,0]**(2-1)*(1-x[...,0])**(x[...,2]-1) * x[...,1]**(2-1)*(1-x[...,1])**(x[...,3]-1), basis = basis, basis_conditioned= basis_cond, variable_names=[&#39;x1&#39;,&#39;x2&#39;], conditioned_variable_names=[&#39;beta1&#39;,&#39;beta2&#39;] )
            ```
        

        Args:
            pdf (_type_): _description_
            basis (_type_): _description_
            basis_conditioned (list, optional): _description_. Defaults to [].
            variable_names (list, optional): _description_. Defaults to [].
            conditioned_variable_names (list, optional): _description_. Defaults to [].
            eps (_type_, optional): _description_. Defaults to 1e-10.

        Returns:
            _type_: _description_
        &#34;&#34;&#34;
        xs = tntt.meshgrid([tn.tensor(b.interpolation_pts[0]) for b in basis]+[tn.tensor(b.interpolation_pts[0]) for b in basis_conditioned])
        Ms = tntt.rank1TT([tn.tensor(np.linalg.inv(b.interpolation_pts[1])) for b in basis]+[tn.tensor(np.linalg.inv(b.interpolation_pts[1])) for b in basis_conditioned])
        dofs = Ms @  tntt.interpolate.function_interpolate(pdf, xs, eps=eps)

        p = pdfTT(basis, basis_conditioned = basis_conditioned, variable_names = variable_names, conditioned_variable_names = conditioned_variable_names, dofs = dofs)
        p.normalize()

        return p

    def __repr__(self):
        &#34;&#34;&#34;
        Offer a srtring representation of the instance.        

        Returns:
            str: the representation.
        &#34;&#34;&#34;
        s = &#34;Probability density function:\n&#34;
        s+= &#34;p(&#34; + &#34;,&#34;.join(self.__variable_names) 
        if self.__dc == 0 :
            s += &#34;)\n&#34;
        else:
            s +=&#34;|&#34; + &#34;,&#34;.join(self.__conditioned_variable_names)+&#34;)\n&#34;
        s+= &#34;\nBasis:\n&#34; + &#34;\n&#34;.join([str(b) for b in self.__basis]+[str(b) for b in self.__basis_cond])
        s+= &#34;\n\nDoF:\n&#34;+repr(self.__tt)

        return s

        
    def normalize(self):
        &#34;&#34;&#34;
        Normalize the PDF.
        &#34;&#34;&#34;
        int_tt = tntt.rank1TT([tn.tensor(b.int) for b in self.__basis ])

        if self.__dc&gt;0:
            int_tt = int_tt ** tntt.ones(self.__Nc)

        Z = (self.__tt * int_tt).sum(list(range(self.__d)))
        if self.__dc&gt;0: 
            Z = tntt.ones(self.__N) ** Z

        self.__tt = self.__tt/Z

    @property
    def Z(self):
        &#34;&#34;&#34;
        Computes the normalization constant. No normalization is performed.

        Returns:
            Union[torchtt.TT,float]: the normalization constant. In case of conditioned RVs, a `torchtt.TT` instance is returned.
        &#34;&#34;&#34;
        int_tt = tntt.rank1TT([tn.tensor(b.int) for b in self.__basis ])

        if self.__dc&gt;0:
            int_tt = int_tt ** tntt.ones(self.__Nc)

        Z = (self.__tt * int_tt).sum(list(range(self.__d)))
        if self.__dc&gt;0: 
            Z = tntt.ones(self.__N) ** Z
        
        return Z


    def expected_value(self):
        &#34;&#34;&#34;
        Compute the expected value

        Returns:
            Union[list[torchtt.TT],list[float]]: _description_
        &#34;&#34;&#34;
        E = []
        for i in range(self.__d):
            pts, ws = self.__basis[i].integration_points(2)
            temp = tn.einsum(&#39;ij,j-&gt;i&#39;,tn.tensor(self.__basis[i](pts) * pts) ,tn.tensor(ws))

            tmp = tntt.rank1TT([tn.tensor(self.__basis[k].int) if k!=i else temp for k in range(self.__d)])
            if self.__dc &gt; 0:
                tmp = tmp ** tntt.ones(self.__Nc)
                E.append( pdfTT([], self.__basis_cond, [], self.__conditioned_variable_names, (tmp*self.__tt).sum(list(range(self.__d))) ) )
            else:
                E.append( (tmp*self.__tt).sum() )
            
        return E
    
    def covariance_matrix(self):
        &#34;&#34;&#34;
        Compute the covariance matrix.
        Currently no conditioned RVs are accepted!!!
        
        Returns:
            torch.tensor: the covariance matrix.
        &#34;&#34;&#34;
        C = tn.zeros((self.__d,self.__d))
        E = self.expected_value()
        
        Pts = [b.integration_points(4)[0] for b in self.__basis]
        Ws = [b.integration_points(4)[1] for b in self.__basis]
        Bs = [self.__basis[k](Pts[k]) for k in range(self.__d)]
        w_tt = tntt.rank1TT([tn.tensor(w) for w in Ws])
        
        for i in range(self.__d):
            
            for j in range(i,self.__d):
                
                Iop = tntt.rank1TT([ tn.tensor(Bs[k]*(Pts[k] if k==i else 1)*(Pts[k] if k==j else 1)) for k in range(self.__d)])
                
                C[i,j] = tntt.dot( Iop.t() @ self.__tt , w_tt ) - E[i]*E[j]
            
        return C
    
    def marginal(self,mask):
        &#34;&#34;&#34;
        Compute the marginal w.r.t. dimensions given as the mask.
        Currently does not work for conditioned RVs.
        
        Args:
            mask (list[int]): the dimensions. The dimensions are numbered from 0.

        Returns:
            pdfTT: the resultimg pdf.
        &#34;&#34;&#34;
        ints = [tn.tensor(self.basis[k].int) if k in mask else tn.ones([self.basis[k].dim]) for k in range(self.__d)]
        
        basis_new = [self.__basis[k] for k in range(self.__d) if not k in mask]
        variable_names_new = [self.__variable_names[k] for k in range(self.__d) if not k in mask]
        tt_new = (self.dofs * tntt.rank1TT(ints)).sum(mask)
        pdf_new = pdfTT(basis_new, variable_names=variable_names_new, dofs = tt_new)
        pdf_new.normalize()
        return pdf_new
    
     
    
    def round(self,eps=1e-12,rmax=9999):
        &#34;&#34;&#34;
        Round the TT degrees of freedom.
        

        Args:
            eps (float, optional): the epsilon accuracy. Defaults to 1e-12.
            rmax (int, optional): the maximum rank. Defaults to 9999.
        &#34;&#34;&#34;
        self.__tt = self.__tt.round(eps,rmax)
    
    def __call__(self,x):
        
        beval = tntt.rank1TT([tn.tensor(self.__basis[i](x[...,i])).T for i in range(self.__d)]+[tn.tensor(self.__basis_cond[i](x[...,i+self.__d])).T for i in range(self.__dc)])             

        return beval @ self.__tt if beval.is_ttm else tntt.dot(beval, self.__tt)
        
    def __getitem__(self, items):
        &#34;&#34;&#34;
        Evaluate the PDF instance. Following possible ways of using the subscript operator are possible:
         
         - For a conditional PDF, the `Ellipsis` can be used to evaluate the variables we condition on. As an example, for the `pdfTT` instance `p` representing the conditional `p(a,b|c,d)`, the expression `p[...,1,2]` represents the PDF `p(a,b|c=1,d=2)`.
         - If only torcht.tensor of float instances are provided, the result is the tensor that results when the PDF is evaluated on the given tensor-product grid.

        Args:
            items (tuple): _description_

        Returns:
            _type_: _description_
        &#34;&#34;&#34;
        
#         if items[0] == Ellipsis:
#             idx = [slice(None,None,None)]*(self.__d+self.__dc-len(items))
#         else:
#             idx = items
# 
#         if len(idx) != self.__d+self.__dc:
#             raise Exception(&#34;The number of dimensions does not match with the number of inputs.&#34;)
# 
#         for i in range(len(idx)):
#             if isinstance(idx[i], slice) and idx[i].start == None and idx[i].stop==None and idx[i].step == None:
        if not isinstance(items, tuple):
            items = (items,)
        if items[0] is Ellipsis:
            bevals = tntt.ones(self.__N) ** tntt.rank1TT([b(i) for i,b in zip(items[1:], self.__basis_cond)])
            dofs = (self.dofs*bevals).sum(list(range(self.__d,self.__d+self.__dc)))
            return pdfTT(self.__basis, [], self.__variable_names, [], dofs)
        else:
            bt = self.__basis + self.__basis_cond
            bevals = tntt.rank1TT([tn.tensor(bt[i](items[i]).T.reshape([-1,bt[i].dim])) for i in range(len(bt))])
            return bevals @ self.dofs

                

    def __pow__(self,other):
        &#34;&#34;&#34;
        Joint of 2 independent PDFs:
        p(x1,...,xn,y1,...,ym) = p(x1,...,xn) * p(y1,...,ym)

        Args:
            other (pdfTT): the second pdf

        Returns:
            pdfTT: the resulting pdf
        &#34;&#34;&#34;
        basis_new = self.basis + other.basis
        variable_names = self.variable_names + other.variable_names

        pdf = pdfTT(basis_new, variable_names=variable_names, dofs = self.dofs ** other.dofs)
        pdf.normalize()

        return pdf
                </code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="TTCME.pdf.BetaPdfTT"><code class="name flex">
<span>def <span class="ident">BetaPdfTT</span></span>(<span>N: list, alphas: list, betas: list)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the PDF for independent beta distributed RVs. </p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>N</code></strong> :&ensp;<code>list[int]</code></dt>
<dd>the size of the univariate bases.</dd>
<dt><strong><code>alphas</code></strong> :&ensp;<code>list[float]</code></dt>
<dd>list of the values of alpha.</dd>
<dt><strong><code>betas</code></strong> :&ensp;<code>list[float]</code></dt>
<dd>list of values of beta.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="TTCME.pdf.pdfTT" href="#TTCME.pdf.pdfTT">pdfTT</a></code></dt>
<dd>the pdf instance.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def BetaPdfTT(N : list[int], alphas: list[float], betas: list[float]):
    &#34;&#34;&#34;
    Compute the PDF for independent beta distributed RVs. 

    Args:
        N (list[int]): the size of the univariate bases.
        alphas (list[float]): list of the values of alpha.
        betas (list[float]): list of values of beta.

    Returns:
        pdfTT: the pdf instance.
    &#34;&#34;&#34;
    
    basis = [BSplineBasis(n,[0,1],2) for n in N]
    vects = []
    pdf_g = lambda x,alpha,beta: x**(alpha-1)*(1-x)**(beta-1)
    for i in range(len(N)):
        p, M = basis[i].interpolation_pts
        vects.append(tn.tensor(np.linalg.solve(M,pdf_g(p,alphas[i],betas[i]))))

    pdf = pdfTT(basis,[],[],[],tntt.rank1TT(vects)) 
    pdf.normalize()
    return pdf</code></pre>
</details>
</dd>
<dt id="TTCME.pdf.GammaPDF"><code class="name flex">
<span>def <span class="ident">GammaPDF</span></span>(<span>alphas, betas, basis, variable_names=[])</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the PDF for multivariate distribution of independent gammas. </p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>alphas</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>the alphas of the distribution.</dd>
<dt><strong><code>betas</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>the betas of the distribution. </dd>
<dt><strong><code>basis</code></strong> :&ensp;<code>list[<a title="TTCME.basis.UnivariateBasis" href="basis.html#TTCME.basis.UnivariateBasis">UnivariateBasis</a>]</code></dt>
<dd>list of the bases.</dd>
<dt><strong><code>variable_names</code></strong> :&ensp;<code>list[str]</code>, optional</dt>
<dd>the variable names. Defaults to [].</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="TTCME.pdf.pdfTT" href="#TTCME.pdf.pdfTT">pdfTT</a></code></dt>
<dd>the PDF instance.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def GammaPDF(alphas, betas, basis, variable_names = []):
    &#34;&#34;&#34;
    Compute the PDF for multivariate distribution of independent gammas. 

    Args:
        alphas (numpy.array): the alphas of the distribution.
        betas (numpy.array): the betas of the distribution. 
        basis (list[TTCME.basis.UnivariateBasis]): list of the bases.
        variable_names (list[str], optional): the variable names. Defaults to [].

    Returns:
        pdfTT: the PDF instance.
    &#34;&#34;&#34;
    pdf = pdfTT(basis, variable_names = variable_names)
    # print(basis.interpolate(lambda x : x**(alpha-1) * np.exp(-beta*x)))
    tts = []
    for i in range(len(basis)):
        p, M = basis[i].interpolation_pts
        f = lambda x : x**(alphas[i]-1) * np.exp(-betas[i]*x)
        tts.append(tn.tensor(np.linalg.solve(M,f(p))))
    pdf.dofs = tntt.rank1TT(tts)
    pdf.normalize()        
    return pdf</code></pre>
</details>
</dd>
<dt id="TTCME.pdf.SingularPMF"><code class="name flex">
<span>def <span class="ident">SingularPMF</span></span>(<span>N, I, variable_names=[])</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def SingularPMF(N,I, variable_names = []):
    
    dofs =  tntt.rank1TT([tn.eye(N[i])[I[i],:] for i in range(len(N))])
    basis = [DiracDeltaBase(n) for n in N]

    return pdfTT(basis, [], variable_names, [], dofs)</code></pre>
</details>
</dd>
<dt id="TTCME.pdf.UniformPDF"><code class="name flex">
<span>def <span class="ident">UniformPDF</span></span>(<span>basis, variable_names=[])</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the PDF for multivariate uniform RVs. </p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>basis</code></strong> :&ensp;<code>list[<a title="TTCME.basis.UnivariateBasis" href="basis.html#TTCME.basis.UnivariateBasis">UnivariateBasis</a>]</code></dt>
<dd>list of the bases.</dd>
<dt><strong><code>variable_names</code></strong> :&ensp;<code>list[str]</code>, optional</dt>
<dd>the variable names. Defaults to [].</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="TTCME.pdf.pdfTT" href="#TTCME.pdf.pdfTT">pdfTT</a></code></dt>
<dd>the PDF instance.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def UniformPDF( basis, variable_names = []):
    &#34;&#34;&#34;
    Compute the PDF for multivariate uniform RVs. 

    Args:
        basis (list[TTCME.basis.UnivariateBasis]): list of the bases.
        variable_names (list[str], optional): the variable names. Defaults to [].

    Returns:
        pdfTT: the PDF instance.
    &#34;&#34;&#34;
    
    pdf = pdfTT(basis, variable_names = variable_names)
    # print(basis.interpolate(lambda x : x**(alpha-1) * np.exp(-beta*x)))
    tts = []
    for i in range(len(basis)):
        p, M = basis[i].interpolation_pts
        f = lambda x : x*0+1.0
        tts.append(tn.tensor(np.linalg.solve(M,f(p))))
    pdf.update(tntt.rank1TT(tts))
    pdf.normalize()        
    return pdf</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="TTCME.pdf.GaussianObservation"><code class="flex name class">
<span>class <span class="ident">GaussianObservation</span></span>
<span>(</span><span>N, sigmas)</span>
</code></dt>
<dd>
<div class="desc"><p>Implements the Gaussian observation operator class for the CME:</p>
<p><span><span class="MathJax_Preview"> p(\mathbf{y} | \mathbf{x}) \sim \prod\limits_{k=1}^d \exp(-\frac{1}{2}\frac{(y_k-x_k)^2}{\sigma_k^2})</span><script type="math/tex; mode=display"> p(\mathbf{y} | \mathbf{x}) \sim \prod\limits_{k=1}^d \exp(-\frac{1}{2}\frac{(y_k-x_k)^2}{\sigma_k^2})</script></span></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>N</code></strong> :&ensp;<code>list[int]</code></dt>
<dd>the state truncation.</dd>
<dt><strong><code>sigmas</code></strong> :&ensp;<code>list[float]</code></dt>
<dd>the variances of the independent Gaussians.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GaussianObservation:

    def __init__(self, N, sigmas):
        &#34;&#34;&#34;
        Implements the Gaussian observation operator class for the CME:

        $$ p(\mathbf{y} | \mathbf{x}) \sim \prod\limits_{k=1}^d \exp(-\\frac{1}{2}\\frac{(y_k-x_k)^2}{\sigma_k^2})$$
        
        Args:
            N (list[int]): the state truncation.
            sigmas (list[float]): the variances of the independent Gaussians.
        &#34;&#34;&#34;
        self.__N = N
        self.__sigmas = sigmas
        
    def likelihood(self, observation):
        &#34;&#34;&#34;
        Computes the likelihood given an observation and returns it a a tensor in the TT format.
        The returned tensor is
        
        $$ \mathsf{p}^{(\\text{obs})}_{i_1...i_d} = p(\\mathbf{y} | \\mathbf{x} = (i_1,...,i_d) ),$$ 
        
        where \( \\mathbf{y} \) is the given observation.

        Args:
            observation (numpy.array): the observation vector.

        Returns:
            torchtt.TT: the likelihood.
        &#34;&#34;&#34;
        noise_model = lambda x,y,s :  np.exp(-(y-x)**2/(2*s**2))

        tens = tntt.rank1TT([tn.tensor(noise_model(np.arange(self.__N[i]),observation[i],self.__sigmas[i])) for i in range(len(self.__N))])

        return tens

    @staticmethod
    def add_noise(sample, sigmas ):
        &#34;&#34;&#34;
        Adds noise to the given sample accordinf to the pdf.

        Args:
            sample (np.array): m x d array containing the number of eact species at disrete time steps. d is the number of species and m is the number of observations.
            sigmas (list[float]): the sigmas of the normal random number generator.

        Returns:
            np.array: the resulting sample with noise.
        &#34;&#34;&#34;
        sample += np.random.normal(scale = sigmas, size=sample.shape)
        return sample</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="TTCME.pdf.GaussianObservation.add_noise"><code class="name flex">
<span>def <span class="ident">add_noise</span></span>(<span>sample, sigmas)</span>
</code></dt>
<dd>
<div class="desc"><p>Adds noise to the given sample accordinf to the pdf.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sample</code></strong> :&ensp;<code>np.array</code></dt>
<dd>m x d array containing the number of eact species at disrete time steps. d is the number of species and m is the number of observations.</dd>
<dt><strong><code>sigmas</code></strong> :&ensp;<code>list[float]</code></dt>
<dd>the sigmas of the normal random number generator.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.array</code></dt>
<dd>the resulting sample with noise.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def add_noise(sample, sigmas ):
    &#34;&#34;&#34;
    Adds noise to the given sample accordinf to the pdf.

    Args:
        sample (np.array): m x d array containing the number of eact species at disrete time steps. d is the number of species and m is the number of observations.
        sigmas (list[float]): the sigmas of the normal random number generator.

    Returns:
        np.array: the resulting sample with noise.
    &#34;&#34;&#34;
    sample += np.random.normal(scale = sigmas, size=sample.shape)
    return sample</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="TTCME.pdf.GaussianObservation.likelihood"><code class="name flex">
<span>def <span class="ident">likelihood</span></span>(<span>self, observation)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the likelihood given an observation and returns it a a tensor in the TT format.
The returned tensor is</p>
<p><span><span class="MathJax_Preview"> \mathsf{p}^{(\text{obs})}_{i_1...i_d} = p(\mathbf{y} | \mathbf{x} = (i_1,...,i_d) ),</span><script type="math/tex; mode=display"> \mathsf{p}^{(\text{obs})}_{i_1...i_d} = p(\mathbf{y} | \mathbf{x} = (i_1,...,i_d) ),</script></span> </p>
<p>where <span><span class="MathJax_Preview"> \mathbf{y} </span><script type="math/tex"> \mathbf{y} </script></span> is the given observation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>observation</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>the observation vector.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torchtt.TT</code></dt>
<dd>the likelihood.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def likelihood(self, observation):
    &#34;&#34;&#34;
    Computes the likelihood given an observation and returns it a a tensor in the TT format.
    The returned tensor is
    
    $$ \mathsf{p}^{(\\text{obs})}_{i_1...i_d} = p(\\mathbf{y} | \\mathbf{x} = (i_1,...,i_d) ),$$ 
    
    where \( \\mathbf{y} \) is the given observation.

    Args:
        observation (numpy.array): the observation vector.

    Returns:
        torchtt.TT: the likelihood.
    &#34;&#34;&#34;
    noise_model = lambda x,y,s :  np.exp(-(y-x)**2/(2*s**2))

    tens = tntt.rank1TT([tn.tensor(noise_model(np.arange(self.__N[i]),observation[i],self.__sigmas[i])) for i in range(len(self.__N))])

    return tens</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="TTCME.pdf.LogNormalObservation"><code class="flex name class">
<span>class <span class="ident">LogNormalObservation</span></span>
<span>(</span><span>N, sigmas)</span>
</code></dt>
<dd>
<div class="desc"><p>Implements the Gaussian observation operator class for the CME:</p>
<p><span><span class="MathJax_Preview"> p(\mathbf{y} | \mathbf{x}) \sim \prod\limits_{k=1}^d \frac{1}{y_k \sigma_k \sqrt{2 \pi}} \exp(-\frac{1}{2}\frac{(\log{y_k}-\log{(x_k+1)})^2}{\sigma_k^2})</span><script type="math/tex; mode=display"> p(\mathbf{y} | \mathbf{x}) \sim \prod\limits_{k=1}^d \frac{1}{y_k \sigma_k \sqrt{2 \pi}} \exp(-\frac{1}{2}\frac{(\log{y_k}-\log{(x_k+1)})^2}{\sigma_k^2})</script></span></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>N</code></strong> :&ensp;<code>list[int]</code></dt>
<dd>the state truncation.</dd>
<dt><strong><code>sigmas</code></strong> :&ensp;<code>list[float]</code></dt>
<dd>the variances of the independent Gaussians.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LogNormalObservation:

    def __init__(self, N, sigmas):
        &#34;&#34;&#34;
        Implements the Gaussian observation operator class for the CME:

        $$ p(\\mathbf{y} | \\mathbf{x}) \sim \prod\limits_{k=1}^d \\frac{1}{y_k \sigma_k \sqrt{2 \pi}} \exp(-\\frac{1}{2}\\frac{(\log{y_k}-\log{(x_k+1)})^2}{\sigma_k^2})$$
        
        Args:
            N (list[int]): the state truncation.
            sigmas (list[float]): the variances of the independent Gaussians.
        &#34;&#34;&#34;
        self.__N = N
        self.__sigmas = sigmas
        
    def likelihood(self, observation):
        &#34;&#34;&#34;
        Computes the likelihood given an observation and returns it a a tensor in the TT format.
        The returned tensor is
        
        $$ \mathsf{p}^{(\\text{obs})}_{i_1...i_d} = p(\\mathbf{y} | \\mathbf{x} = (i_1,...,i_d) ),$$ 
        
        where \( \\mathbf{y} \) is the given observation.

        Args:
            observation (numpy.array): the observation vector.

        Returns:
            torchtt.TT: the likelihood.
        &#34;&#34;&#34;
        noise_model = lambda x,y,s : 1/(y*s*np.sqrt(2*np.pi)) * np.exp(-(np.log(y)-np.log(x+1))**2/(2*s**2))

        tens = tntt.rank1TT([tn.tensor(noise_model(np.arange(self.__N[i]),observation[i],self.__sigmas[i])) for i in range(len(self.__N))])

        return tens

    @staticmethod
    def add_noise(sample, sigmas):
        &#34;&#34;&#34;
        Adds noise to the given sample according to the pdf.

        Args:
            sample (np.array): m x d array containing the number of eact species at disrete time steps. d is the number of species and m is the number of observations.
            sigmas (list[float]): the sigmas of the normal random number generator.

        Returns:
            np.array: the resulting sample with noise.
        &#34;&#34;&#34;
        lst = [ np.random.lognormal(np.log(sample[:,i]+1),sigmas[i]).reshape([-1,1]) for i in range(len(sigmas)) ]
        
        sample = np.hstack(tuple(lst))
        
        return sample</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="TTCME.pdf.LogNormalObservation.add_noise"><code class="name flex">
<span>def <span class="ident">add_noise</span></span>(<span>sample, sigmas)</span>
</code></dt>
<dd>
<div class="desc"><p>Adds noise to the given sample according to the pdf.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>sample</code></strong> :&ensp;<code>np.array</code></dt>
<dd>m x d array containing the number of eact species at disrete time steps. d is the number of species and m is the number of observations.</dd>
<dt><strong><code>sigmas</code></strong> :&ensp;<code>list[float]</code></dt>
<dd>the sigmas of the normal random number generator.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.array</code></dt>
<dd>the resulting sample with noise.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def add_noise(sample, sigmas):
    &#34;&#34;&#34;
    Adds noise to the given sample according to the pdf.

    Args:
        sample (np.array): m x d array containing the number of eact species at disrete time steps. d is the number of species and m is the number of observations.
        sigmas (list[float]): the sigmas of the normal random number generator.

    Returns:
        np.array: the resulting sample with noise.
    &#34;&#34;&#34;
    lst = [ np.random.lognormal(np.log(sample[:,i]+1),sigmas[i]).reshape([-1,1]) for i in range(len(sigmas)) ]
    
    sample = np.hstack(tuple(lst))
    
    return sample</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="TTCME.pdf.LogNormalObservation.likelihood"><code class="name flex">
<span>def <span class="ident">likelihood</span></span>(<span>self, observation)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the likelihood given an observation and returns it a a tensor in the TT format.
The returned tensor is</p>
<p><span><span class="MathJax_Preview"> \mathsf{p}^{(\text{obs})}_{i_1...i_d} = p(\mathbf{y} | \mathbf{x} = (i_1,...,i_d) ),</span><script type="math/tex; mode=display"> \mathsf{p}^{(\text{obs})}_{i_1...i_d} = p(\mathbf{y} | \mathbf{x} = (i_1,...,i_d) ),</script></span> </p>
<p>where <span><span class="MathJax_Preview"> \mathbf{y} </span><script type="math/tex"> \mathbf{y} </script></span> is the given observation.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>observation</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>the observation vector.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torchtt.TT</code></dt>
<dd>the likelihood.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def likelihood(self, observation):
    &#34;&#34;&#34;
    Computes the likelihood given an observation and returns it a a tensor in the TT format.
    The returned tensor is
    
    $$ \mathsf{p}^{(\\text{obs})}_{i_1...i_d} = p(\\mathbf{y} | \\mathbf{x} = (i_1,...,i_d) ),$$ 
    
    where \( \\mathbf{y} \) is the given observation.

    Args:
        observation (numpy.array): the observation vector.

    Returns:
        torchtt.TT: the likelihood.
    &#34;&#34;&#34;
    noise_model = lambda x,y,s : 1/(y*s*np.sqrt(2*np.pi)) * np.exp(-(np.log(y)-np.log(x+1))**2/(2*s**2))

    tens = tntt.rank1TT([tn.tensor(noise_model(np.arange(self.__N[i]),observation[i],self.__sigmas[i])) for i in range(len(self.__N))])

    return tens</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="TTCME.pdf.pdfTT"><code class="flex name class">
<span>class <span class="ident">pdfTT</span></span>
<span>(</span><span>basis, basis_conditioned=[], variable_names=[], conditioned_variable_names=[], dofs=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Probability density function approximation using a tensor-product basis: <br>
<span><span class="MathJax_Preview"> p(x_1,...,x_d|y_1,...,y_{d'})
= \sum\limits_{k_1,...,k_d,n_1,...,n_{d'}} \mathsf{d}_{k_1...k_dn_1...n_d} b^{(1)}(x_1) \cdots b^{(d)}(x_d) b'^{(1)}(y_1) \cdots b'^{(d')}(y_{d'}),</span><script type="math/tex; mode=display"> p(x_1,...,x_d|y_1,...,y_{d'})
= \sum\limits_{k_1,...,k_d,n_1,...,n_{d'}} \mathsf{d}_{k_1...k_dn_1...n_d} b^{(1)}(x_1) \cdots b^{(d)}(x_d) b'^{(1)}(y_1) \cdots b'^{(d')}(y_{d'}),</script></span>
where <span><span class="MathJax_Preview"> \mathsf{d}</span><script type="math/tex"> \mathsf{d}</script></span> is the DoF tensor and is represented in the TT format.</p>
<p>Using the subscript operator, the pdf can be evaluated at given values. Following possible ways of using the subscript operator are possible:</p>
<ul>
<li>For a conditional PDF, the <code>Ellipsis</code> can be used to evaluate the variables we condition on. As an example, for the <code><a title="TTCME.pdf.pdfTT" href="#TTCME.pdf.pdfTT">pdfTT</a></code> instance <code>p</code> representing the conditional <code>p(a,b|c,d)</code>, the expression <code>p[&hellip;,1,2]</code> represents the PDF <code>p(a,b|c=1,d=2)</code>.</li>
<li>If only torcht.tensor of float instances are provided, the result is the tensor that results when the PDF is evaluated on the given tensor-product grid.</li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>basis</code></strong> :&ensp;<code>list[UnivariateBasis]</code></dt>
<dd>the univariate bases used to construct the tensor-product basis (( b^{(k)} ) in the formula above). </dd>
<dt><strong><code>basis_conditioned</code></strong> :&ensp;<code>list[UnivariateBasis]</code>, optional</dt>
<dd>the univariate bases in case a conditional is used (corresponding to the variables right of the conditioned sign). In the equation above are denoted by <span><span class="MathJax_Preview"> b'^{(k)} </span><script type="math/tex"> b'^{(k)} </script></span>. Defaults to [].</dd>
<dt><strong><code>variable_names</code></strong> :&ensp;<code>list[str]</code>, optional</dt>
<dd>the name of variables as a list of strings. Defaults to [].</dd>
<dt><strong><code>conditioned_variable_names</code></strong> :&ensp;<code>list[str]</code>, optional</dt>
<dd>the names of the bariables that we condition on. Defaults to [].</dd>
<dt><strong><code>dofs</code></strong> :&ensp;<code>torchtt.TT</code>, optional</dt>
<dd>the dofs in the TT format. If None is provided an uniform PDF is created. Defaults to None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class pdfTT():
    def __init__(self, basis, basis_conditioned = [], variable_names = [], conditioned_variable_names = [], dofs = None):
        &#34;&#34;&#34;
        Probability density function approximation using a tensor-product basis:   
        $$ p(x_1,...,x_d|y_1,...,y_{d&#39;})  = \\sum\\limits_{k_1,...,k_d,n_1,...,n_{d&#39;}} \\mathsf{d}_{k_1...k_dn_1...n_d} b^{(1)}(x_1) \\cdots b^{(d)}(x_d) b&#39;^{(1)}(y_1) \\cdots b&#39;^{(d&#39;)}(y_{d&#39;}),$$
        where \( \\mathsf{d}\) is the DoF tensor and is represented in the TT format.
        
        Using the subscript operator, the pdf can be evaluated at given values. Following possible ways of using the subscript operator are possible:
         
         - For a conditional PDF, the `Ellipsis` can be used to evaluate the variables we condition on. As an example, for the `pdfTT` instance `p` representing the conditional `p(a,b|c,d)`, the expression `p[...,1,2]` represents the PDF `p(a,b|c=1,d=2)`.
         - If only torcht.tensor of float instances are provided, the result is the tensor that results when the PDF is evaluated on the given tensor-product grid.


        Args:
            basis (list[UnivariateBasis]): the univariate bases used to construct the tensor-product basis (\( b^{(k)} \) in the formula above). 
            basis_conditioned (list[UnivariateBasis], optional): the univariate bases in case a conditional is used (corresponding to the variables right of the conditioned sign). In the equation above are denoted by \( b&#39;^{(k)} \). Defaults to [].
            variable_names (list[str], optional): the name of variables as a list of strings. Defaults to [].
            conditioned_variable_names (list[str], optional): the names of the bariables that we condition on. Defaults to [].
            dofs (torchtt.TT, optional): the dofs in the TT format. If None is provided an uniform PDF is created. Defaults to None.
        &#34;&#34;&#34;
        self.__d = len(basis)
        self.__dc = len(basis_conditioned) 
        self.__variable_names = variable_names
        self.__conditioned_variable_names = conditioned_variable_names
        self.__N = [b.dim for b in basis]
        self.__Nc = [b.dim for b in basis_conditioned]
        self.__basis = basis.copy()
        self.__basis_cond = basis_conditioned.copy()

        if dofs == None:
            self.__tt = tntt.ones(self.__N+self.__Nc)
        else:
            self.__tt = dofs.clone()


    def copy(self):
        &#34;&#34;&#34;
        Create a copy of the `pdfTT` instance.

        Returns:
            pdfTT: the copy.
        &#34;&#34;&#34;
        return copy.deepcopy(self)

    @property
    def basis(self):
        &#34;&#34;&#34;
        list[UnivariateBasis]: the basis used.
        &#34;&#34;&#34;
        return self.__basis.copy()

    @property
    def basis_conditioned(self):
        &#34;&#34;&#34;
        list[UnivariateBasis]: the basis used for the conditioned variables.
        &#34;&#34;&#34;
        return self.__basis_cond.copy()

    @property
    def variable_names(self):
        &#34;&#34;&#34;
        list[str]: the name of the variables.
        &#34;&#34;&#34;
        return self.__variable_names.copy()

    @property
    def conditioned_variable_names(self):
        &#34;&#34;&#34; 
        list[str]: the name of the variables we condition on.
        &#34;&#34;&#34;
        return self.__conditioned_variable_names.copy()

    @property
    def dofs(self):
        &#34;&#34;&#34;
        torchtt.TT: the dofs tensor in the TT format.
        &#34;&#34;&#34;
        return self.__tt

    @dofs.setter 
    def dofs(self, value):
        if not isinstance(value, tntt.TT):
            raise Exception(&#34;DOFS have to be in the TT format&#34;)

        if value.is_ttm or value.N != self.__tt.N:
            raise Exception(&#34;Dimensions must match&#34;)

        self.__tt = value
        
    @classmethod
    def interpoalte(cls, pdf, basis, basis_conditioned = [], variable_names = [], conditioned_variable_names = [], eps = 1e-10):
        &#34;&#34;&#34;
        Interpolate a pdf using the given basis.
        
        Example:
            ```
            import TTCME
            basis = [TTCME.basis.BSplineBasis(64,[0,1],2), TTCME.basis.BSplineBasis(64,[0,1],2)]
            basis_cond = [TTCME.basis.BSplineBasis(32,[2,3],2), TTCME.basis.BSplineBasis(43,[2, 3],2)]
            pdf_c = TTCME.pdf.pdfTT.interpoalte(pdf = lambda x: x[...,0]**(2-1)*(1-x[...,0])**(x[...,2]-1) * x[...,1]**(2-1)*(1-x[...,1])**(x[...,3]-1), basis = basis, basis_conditioned= basis_cond, variable_names=[&#39;x1&#39;,&#39;x2&#39;], conditioned_variable_names=[&#39;beta1&#39;,&#39;beta2&#39;] )
            ```
        

        Args:
            pdf (_type_): _description_
            basis (_type_): _description_
            basis_conditioned (list, optional): _description_. Defaults to [].
            variable_names (list, optional): _description_. Defaults to [].
            conditioned_variable_names (list, optional): _description_. Defaults to [].
            eps (_type_, optional): _description_. Defaults to 1e-10.

        Returns:
            _type_: _description_
        &#34;&#34;&#34;
        xs = tntt.meshgrid([tn.tensor(b.interpolation_pts[0]) for b in basis]+[tn.tensor(b.interpolation_pts[0]) for b in basis_conditioned])
        Ms = tntt.rank1TT([tn.tensor(np.linalg.inv(b.interpolation_pts[1])) for b in basis]+[tn.tensor(np.linalg.inv(b.interpolation_pts[1])) for b in basis_conditioned])
        dofs = Ms @  tntt.interpolate.function_interpolate(pdf, xs, eps=eps)

        p = pdfTT(basis, basis_conditioned = basis_conditioned, variable_names = variable_names, conditioned_variable_names = conditioned_variable_names, dofs = dofs)
        p.normalize()

        return p

    def __repr__(self):
        &#34;&#34;&#34;
        Offer a srtring representation of the instance.        

        Returns:
            str: the representation.
        &#34;&#34;&#34;
        s = &#34;Probability density function:\n&#34;
        s+= &#34;p(&#34; + &#34;,&#34;.join(self.__variable_names) 
        if self.__dc == 0 :
            s += &#34;)\n&#34;
        else:
            s +=&#34;|&#34; + &#34;,&#34;.join(self.__conditioned_variable_names)+&#34;)\n&#34;
        s+= &#34;\nBasis:\n&#34; + &#34;\n&#34;.join([str(b) for b in self.__basis]+[str(b) for b in self.__basis_cond])
        s+= &#34;\n\nDoF:\n&#34;+repr(self.__tt)

        return s

        
    def normalize(self):
        &#34;&#34;&#34;
        Normalize the PDF.
        &#34;&#34;&#34;
        int_tt = tntt.rank1TT([tn.tensor(b.int) for b in self.__basis ])

        if self.__dc&gt;0:
            int_tt = int_tt ** tntt.ones(self.__Nc)

        Z = (self.__tt * int_tt).sum(list(range(self.__d)))
        if self.__dc&gt;0: 
            Z = tntt.ones(self.__N) ** Z

        self.__tt = self.__tt/Z

    @property
    def Z(self):
        &#34;&#34;&#34;
        Computes the normalization constant. No normalization is performed.

        Returns:
            Union[torchtt.TT,float]: the normalization constant. In case of conditioned RVs, a `torchtt.TT` instance is returned.
        &#34;&#34;&#34;
        int_tt = tntt.rank1TT([tn.tensor(b.int) for b in self.__basis ])

        if self.__dc&gt;0:
            int_tt = int_tt ** tntt.ones(self.__Nc)

        Z = (self.__tt * int_tt).sum(list(range(self.__d)))
        if self.__dc&gt;0: 
            Z = tntt.ones(self.__N) ** Z
        
        return Z


    def expected_value(self):
        &#34;&#34;&#34;
        Compute the expected value

        Returns:
            Union[list[torchtt.TT],list[float]]: _description_
        &#34;&#34;&#34;
        E = []
        for i in range(self.__d):
            pts, ws = self.__basis[i].integration_points(2)
            temp = tn.einsum(&#39;ij,j-&gt;i&#39;,tn.tensor(self.__basis[i](pts) * pts) ,tn.tensor(ws))

            tmp = tntt.rank1TT([tn.tensor(self.__basis[k].int) if k!=i else temp for k in range(self.__d)])
            if self.__dc &gt; 0:
                tmp = tmp ** tntt.ones(self.__Nc)
                E.append( pdfTT([], self.__basis_cond, [], self.__conditioned_variable_names, (tmp*self.__tt).sum(list(range(self.__d))) ) )
            else:
                E.append( (tmp*self.__tt).sum() )
            
        return E
    
    def covariance_matrix(self):
        &#34;&#34;&#34;
        Compute the covariance matrix.
        Currently no conditioned RVs are accepted!!!
        
        Returns:
            torch.tensor: the covariance matrix.
        &#34;&#34;&#34;
        C = tn.zeros((self.__d,self.__d))
        E = self.expected_value()
        
        Pts = [b.integration_points(4)[0] for b in self.__basis]
        Ws = [b.integration_points(4)[1] for b in self.__basis]
        Bs = [self.__basis[k](Pts[k]) for k in range(self.__d)]
        w_tt = tntt.rank1TT([tn.tensor(w) for w in Ws])
        
        for i in range(self.__d):
            
            for j in range(i,self.__d):
                
                Iop = tntt.rank1TT([ tn.tensor(Bs[k]*(Pts[k] if k==i else 1)*(Pts[k] if k==j else 1)) for k in range(self.__d)])
                
                C[i,j] = tntt.dot( Iop.t() @ self.__tt , w_tt ) - E[i]*E[j]
            
        return C
    
    def marginal(self,mask):
        &#34;&#34;&#34;
        Compute the marginal w.r.t. dimensions given as the mask.
        Currently does not work for conditioned RVs.
        
        Args:
            mask (list[int]): the dimensions. The dimensions are numbered from 0.

        Returns:
            pdfTT: the resultimg pdf.
        &#34;&#34;&#34;
        ints = [tn.tensor(self.basis[k].int) if k in mask else tn.ones([self.basis[k].dim]) for k in range(self.__d)]
        
        basis_new = [self.__basis[k] for k in range(self.__d) if not k in mask]
        variable_names_new = [self.__variable_names[k] for k in range(self.__d) if not k in mask]
        tt_new = (self.dofs * tntt.rank1TT(ints)).sum(mask)
        pdf_new = pdfTT(basis_new, variable_names=variable_names_new, dofs = tt_new)
        pdf_new.normalize()
        return pdf_new
    
     
    
    def round(self,eps=1e-12,rmax=9999):
        &#34;&#34;&#34;
        Round the TT degrees of freedom.
        

        Args:
            eps (float, optional): the epsilon accuracy. Defaults to 1e-12.
            rmax (int, optional): the maximum rank. Defaults to 9999.
        &#34;&#34;&#34;
        self.__tt = self.__tt.round(eps,rmax)
    
    def __call__(self,x):
        
        beval = tntt.rank1TT([tn.tensor(self.__basis[i](x[...,i])).T for i in range(self.__d)]+[tn.tensor(self.__basis_cond[i](x[...,i+self.__d])).T for i in range(self.__dc)])             

        return beval @ self.__tt if beval.is_ttm else tntt.dot(beval, self.__tt)
        
    def __getitem__(self, items):
        &#34;&#34;&#34;
        Evaluate the PDF instance. Following possible ways of using the subscript operator are possible:
         
         - For a conditional PDF, the `Ellipsis` can be used to evaluate the variables we condition on. As an example, for the `pdfTT` instance `p` representing the conditional `p(a,b|c,d)`, the expression `p[...,1,2]` represents the PDF `p(a,b|c=1,d=2)`.
         - If only torcht.tensor of float instances are provided, the result is the tensor that results when the PDF is evaluated on the given tensor-product grid.

        Args:
            items (tuple): _description_

        Returns:
            _type_: _description_
        &#34;&#34;&#34;
        
#         if items[0] == Ellipsis:
#             idx = [slice(None,None,None)]*(self.__d+self.__dc-len(items))
#         else:
#             idx = items
# 
#         if len(idx) != self.__d+self.__dc:
#             raise Exception(&#34;The number of dimensions does not match with the number of inputs.&#34;)
# 
#         for i in range(len(idx)):
#             if isinstance(idx[i], slice) and idx[i].start == None and idx[i].stop==None and idx[i].step == None:
        if not isinstance(items, tuple):
            items = (items,)
        if items[0] is Ellipsis:
            bevals = tntt.ones(self.__N) ** tntt.rank1TT([b(i) for i,b in zip(items[1:], self.__basis_cond)])
            dofs = (self.dofs*bevals).sum(list(range(self.__d,self.__d+self.__dc)))
            return pdfTT(self.__basis, [], self.__variable_names, [], dofs)
        else:
            bt = self.__basis + self.__basis_cond
            bevals = tntt.rank1TT([tn.tensor(bt[i](items[i]).T.reshape([-1,bt[i].dim])) for i in range(len(bt))])
            return bevals @ self.dofs

                

    def __pow__(self,other):
        &#34;&#34;&#34;
        Joint of 2 independent PDFs:
        p(x1,...,xn,y1,...,ym) = p(x1,...,xn) * p(y1,...,ym)

        Args:
            other (pdfTT): the second pdf

        Returns:
            pdfTT: the resulting pdf
        &#34;&#34;&#34;
        basis_new = self.basis + other.basis
        variable_names = self.variable_names + other.variable_names

        pdf = pdfTT(basis_new, variable_names=variable_names, dofs = self.dofs ** other.dofs)
        pdf.normalize()

        return pdf</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="TTCME.pdf.pdfTT.interpoalte"><code class="name flex">
<span>def <span class="ident">interpoalte</span></span>(<span>pdf, basis, basis_conditioned=[], variable_names=[], conditioned_variable_names=[], eps=1e-10)</span>
</code></dt>
<dd>
<div class="desc"><p>Interpolate a pdf using the given basis.</p>
<h2 id="example">Example</h2>
<pre><code>import TTCME
basis = [TTCME.basis.BSplineBasis(64,[0,1],2), TTCME.basis.BSplineBasis(64,[0,1],2)]
basis_cond = [TTCME.basis.BSplineBasis(32,[2,3],2), TTCME.basis.BSplineBasis(43,[2, 3],2)]
pdf_c = TTCME.pdf.pdfTT.interpoalte(pdf = lambda x: x[...,0]**(2-1)*(1-x[...,0])**(x[...,2]-1) * x[...,1]**(2-1)*(1-x[...,1])**(x[...,3]-1), basis = basis, basis_conditioned= basis_cond, variable_names=['x1','x2'], conditioned_variable_names=['beta1','beta2'] )
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pdf</code></strong> :&ensp;<code>_type_</code></dt>
<dd><em>description</em></dd>
<dt><strong><code>basis</code></strong> :&ensp;<code>_type_</code></dt>
<dd><em>description</em></dd>
<dt><strong><code>basis_conditioned</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd><em>description</em>. Defaults to [].</dd>
<dt><strong><code>variable_names</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd><em>description</em>. Defaults to [].</dd>
<dt><strong><code>conditioned_variable_names</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd><em>description</em>. Defaults to [].</dd>
<dt><strong><code>eps</code></strong> :&ensp;<code>_type_</code>, optional</dt>
<dd><em>description</em>. Defaults to 1e-10.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>_type_</code></dt>
<dd><em>description</em></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def interpoalte(cls, pdf, basis, basis_conditioned = [], variable_names = [], conditioned_variable_names = [], eps = 1e-10):
    &#34;&#34;&#34;
    Interpolate a pdf using the given basis.
    
    Example:
        ```
        import TTCME
        basis = [TTCME.basis.BSplineBasis(64,[0,1],2), TTCME.basis.BSplineBasis(64,[0,1],2)]
        basis_cond = [TTCME.basis.BSplineBasis(32,[2,3],2), TTCME.basis.BSplineBasis(43,[2, 3],2)]
        pdf_c = TTCME.pdf.pdfTT.interpoalte(pdf = lambda x: x[...,0]**(2-1)*(1-x[...,0])**(x[...,2]-1) * x[...,1]**(2-1)*(1-x[...,1])**(x[...,3]-1), basis = basis, basis_conditioned= basis_cond, variable_names=[&#39;x1&#39;,&#39;x2&#39;], conditioned_variable_names=[&#39;beta1&#39;,&#39;beta2&#39;] )
        ```
    

    Args:
        pdf (_type_): _description_
        basis (_type_): _description_
        basis_conditioned (list, optional): _description_. Defaults to [].
        variable_names (list, optional): _description_. Defaults to [].
        conditioned_variable_names (list, optional): _description_. Defaults to [].
        eps (_type_, optional): _description_. Defaults to 1e-10.

    Returns:
        _type_: _description_
    &#34;&#34;&#34;
    xs = tntt.meshgrid([tn.tensor(b.interpolation_pts[0]) for b in basis]+[tn.tensor(b.interpolation_pts[0]) for b in basis_conditioned])
    Ms = tntt.rank1TT([tn.tensor(np.linalg.inv(b.interpolation_pts[1])) for b in basis]+[tn.tensor(np.linalg.inv(b.interpolation_pts[1])) for b in basis_conditioned])
    dofs = Ms @  tntt.interpolate.function_interpolate(pdf, xs, eps=eps)

    p = pdfTT(basis, basis_conditioned = basis_conditioned, variable_names = variable_names, conditioned_variable_names = conditioned_variable_names, dofs = dofs)
    p.normalize()

    return p</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="TTCME.pdf.pdfTT.Z"><code class="name">var <span class="ident">Z</span></code></dt>
<dd>
<div class="desc"><p>Computes the normalization constant. No normalization is performed.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Union[torchtt.TT,float]</code></dt>
<dd>the normalization constant. In case of conditioned RVs, a <code>torchtt.TT</code> instance is returned.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def Z(self):
    &#34;&#34;&#34;
    Computes the normalization constant. No normalization is performed.

    Returns:
        Union[torchtt.TT,float]: the normalization constant. In case of conditioned RVs, a `torchtt.TT` instance is returned.
    &#34;&#34;&#34;
    int_tt = tntt.rank1TT([tn.tensor(b.int) for b in self.__basis ])

    if self.__dc&gt;0:
        int_tt = int_tt ** tntt.ones(self.__Nc)

    Z = (self.__tt * int_tt).sum(list(range(self.__d)))
    if self.__dc&gt;0: 
        Z = tntt.ones(self.__N) ** Z
    
    return Z</code></pre>
</details>
</dd>
<dt id="TTCME.pdf.pdfTT.basis"><code class="name">var <span class="ident">basis</span></code></dt>
<dd>
<div class="desc"><p>list[UnivariateBasis]: the basis used.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def basis(self):
    &#34;&#34;&#34;
    list[UnivariateBasis]: the basis used.
    &#34;&#34;&#34;
    return self.__basis.copy()</code></pre>
</details>
</dd>
<dt id="TTCME.pdf.pdfTT.basis_conditioned"><code class="name">var <span class="ident">basis_conditioned</span></code></dt>
<dd>
<div class="desc"><p>list[UnivariateBasis]: the basis used for the conditioned variables.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def basis_conditioned(self):
    &#34;&#34;&#34;
    list[UnivariateBasis]: the basis used for the conditioned variables.
    &#34;&#34;&#34;
    return self.__basis_cond.copy()</code></pre>
</details>
</dd>
<dt id="TTCME.pdf.pdfTT.conditioned_variable_names"><code class="name">var <span class="ident">conditioned_variable_names</span></code></dt>
<dd>
<div class="desc"><p>list[str]: the name of the variables we condition on.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def conditioned_variable_names(self):
    &#34;&#34;&#34; 
    list[str]: the name of the variables we condition on.
    &#34;&#34;&#34;
    return self.__conditioned_variable_names.copy()</code></pre>
</details>
</dd>
<dt id="TTCME.pdf.pdfTT.dofs"><code class="name">var <span class="ident">dofs</span></code></dt>
<dd>
<div class="desc"><p>torchtt.TT: the dofs tensor in the TT format.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def dofs(self):
    &#34;&#34;&#34;
    torchtt.TT: the dofs tensor in the TT format.
    &#34;&#34;&#34;
    return self.__tt</code></pre>
</details>
</dd>
<dt id="TTCME.pdf.pdfTT.variable_names"><code class="name">var <span class="ident">variable_names</span></code></dt>
<dd>
<div class="desc"><p>list[str]: the name of the variables.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def variable_names(self):
    &#34;&#34;&#34;
    list[str]: the name of the variables.
    &#34;&#34;&#34;
    return self.__variable_names.copy()</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="TTCME.pdf.pdfTT.copy"><code class="name flex">
<span>def <span class="ident">copy</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Create a copy of the <code><a title="TTCME.pdf.pdfTT" href="#TTCME.pdf.pdfTT">pdfTT</a></code> instance.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="TTCME.pdf.pdfTT" href="#TTCME.pdf.pdfTT">pdfTT</a></code></dt>
<dd>the copy.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def copy(self):
    &#34;&#34;&#34;
    Create a copy of the `pdfTT` instance.

    Returns:
        pdfTT: the copy.
    &#34;&#34;&#34;
    return copy.deepcopy(self)</code></pre>
</details>
</dd>
<dt id="TTCME.pdf.pdfTT.covariance_matrix"><code class="name flex">
<span>def <span class="ident">covariance_matrix</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the covariance matrix.
Currently no conditioned RVs are accepted!!!</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.tensor</code></dt>
<dd>the covariance matrix.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def covariance_matrix(self):
    &#34;&#34;&#34;
    Compute the covariance matrix.
    Currently no conditioned RVs are accepted!!!
    
    Returns:
        torch.tensor: the covariance matrix.
    &#34;&#34;&#34;
    C = tn.zeros((self.__d,self.__d))
    E = self.expected_value()
    
    Pts = [b.integration_points(4)[0] for b in self.__basis]
    Ws = [b.integration_points(4)[1] for b in self.__basis]
    Bs = [self.__basis[k](Pts[k]) for k in range(self.__d)]
    w_tt = tntt.rank1TT([tn.tensor(w) for w in Ws])
    
    for i in range(self.__d):
        
        for j in range(i,self.__d):
            
            Iop = tntt.rank1TT([ tn.tensor(Bs[k]*(Pts[k] if k==i else 1)*(Pts[k] if k==j else 1)) for k in range(self.__d)])
            
            C[i,j] = tntt.dot( Iop.t() @ self.__tt , w_tt ) - E[i]*E[j]
        
    return C</code></pre>
</details>
</dd>
<dt id="TTCME.pdf.pdfTT.expected_value"><code class="name flex">
<span>def <span class="ident">expected_value</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the expected value</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Union[list[torchtt.TT],list[float]]</code></dt>
<dd><em>description</em></dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expected_value(self):
    &#34;&#34;&#34;
    Compute the expected value

    Returns:
        Union[list[torchtt.TT],list[float]]: _description_
    &#34;&#34;&#34;
    E = []
    for i in range(self.__d):
        pts, ws = self.__basis[i].integration_points(2)
        temp = tn.einsum(&#39;ij,j-&gt;i&#39;,tn.tensor(self.__basis[i](pts) * pts) ,tn.tensor(ws))

        tmp = tntt.rank1TT([tn.tensor(self.__basis[k].int) if k!=i else temp for k in range(self.__d)])
        if self.__dc &gt; 0:
            tmp = tmp ** tntt.ones(self.__Nc)
            E.append( pdfTT([], self.__basis_cond, [], self.__conditioned_variable_names, (tmp*self.__tt).sum(list(range(self.__d))) ) )
        else:
            E.append( (tmp*self.__tt).sum() )
        
    return E</code></pre>
</details>
</dd>
<dt id="TTCME.pdf.pdfTT.marginal"><code class="name flex">
<span>def <span class="ident">marginal</span></span>(<span>self, mask)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the marginal w.r.t. dimensions given as the mask.
Currently does not work for conditioned RVs.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>mask</code></strong> :&ensp;<code>list[int]</code></dt>
<dd>the dimensions. The dimensions are numbered from 0.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="TTCME.pdf.pdfTT" href="#TTCME.pdf.pdfTT">pdfTT</a></code></dt>
<dd>the resultimg pdf.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def marginal(self,mask):
    &#34;&#34;&#34;
    Compute the marginal w.r.t. dimensions given as the mask.
    Currently does not work for conditioned RVs.
    
    Args:
        mask (list[int]): the dimensions. The dimensions are numbered from 0.

    Returns:
        pdfTT: the resultimg pdf.
    &#34;&#34;&#34;
    ints = [tn.tensor(self.basis[k].int) if k in mask else tn.ones([self.basis[k].dim]) for k in range(self.__d)]
    
    basis_new = [self.__basis[k] for k in range(self.__d) if not k in mask]
    variable_names_new = [self.__variable_names[k] for k in range(self.__d) if not k in mask]
    tt_new = (self.dofs * tntt.rank1TT(ints)).sum(mask)
    pdf_new = pdfTT(basis_new, variable_names=variable_names_new, dofs = tt_new)
    pdf_new.normalize()
    return pdf_new</code></pre>
</details>
</dd>
<dt id="TTCME.pdf.pdfTT.normalize"><code class="name flex">
<span>def <span class="ident">normalize</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Normalize the PDF.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def normalize(self):
    &#34;&#34;&#34;
    Normalize the PDF.
    &#34;&#34;&#34;
    int_tt = tntt.rank1TT([tn.tensor(b.int) for b in self.__basis ])

    if self.__dc&gt;0:
        int_tt = int_tt ** tntt.ones(self.__Nc)

    Z = (self.__tt * int_tt).sum(list(range(self.__d)))
    if self.__dc&gt;0: 
        Z = tntt.ones(self.__N) ** Z

    self.__tt = self.__tt/Z</code></pre>
</details>
</dd>
<dt id="TTCME.pdf.pdfTT.round"><code class="name flex">
<span>def <span class="ident">round</span></span>(<span>self, eps=1e-12, rmax=9999)</span>
</code></dt>
<dd>
<div class="desc"><p>Round the TT degrees of freedom.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>eps</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>the epsilon accuracy. Defaults to 1e-12.</dd>
<dt><strong><code>rmax</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>the maximum rank. Defaults to 9999.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def round(self,eps=1e-12,rmax=9999):
    &#34;&#34;&#34;
    Round the TT degrees of freedom.
    

    Args:
        eps (float, optional): the epsilon accuracy. Defaults to 1e-12.
        rmax (int, optional): the maximum rank. Defaults to 9999.
    &#34;&#34;&#34;
    self.__tt = self.__tt.round(eps,rmax)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="TTCME" href="index.html">TTCME</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="TTCME.pdf.BetaPdfTT" href="#TTCME.pdf.BetaPdfTT">BetaPdfTT</a></code></li>
<li><code><a title="TTCME.pdf.GammaPDF" href="#TTCME.pdf.GammaPDF">GammaPDF</a></code></li>
<li><code><a title="TTCME.pdf.SingularPMF" href="#TTCME.pdf.SingularPMF">SingularPMF</a></code></li>
<li><code><a title="TTCME.pdf.UniformPDF" href="#TTCME.pdf.UniformPDF">UniformPDF</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="TTCME.pdf.GaussianObservation" href="#TTCME.pdf.GaussianObservation">GaussianObservation</a></code></h4>
<ul class="">
<li><code><a title="TTCME.pdf.GaussianObservation.add_noise" href="#TTCME.pdf.GaussianObservation.add_noise">add_noise</a></code></li>
<li><code><a title="TTCME.pdf.GaussianObservation.likelihood" href="#TTCME.pdf.GaussianObservation.likelihood">likelihood</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="TTCME.pdf.LogNormalObservation" href="#TTCME.pdf.LogNormalObservation">LogNormalObservation</a></code></h4>
<ul class="">
<li><code><a title="TTCME.pdf.LogNormalObservation.add_noise" href="#TTCME.pdf.LogNormalObservation.add_noise">add_noise</a></code></li>
<li><code><a title="TTCME.pdf.LogNormalObservation.likelihood" href="#TTCME.pdf.LogNormalObservation.likelihood">likelihood</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="TTCME.pdf.pdfTT" href="#TTCME.pdf.pdfTT">pdfTT</a></code></h4>
<ul class="">
<li><code><a title="TTCME.pdf.pdfTT.Z" href="#TTCME.pdf.pdfTT.Z">Z</a></code></li>
<li><code><a title="TTCME.pdf.pdfTT.basis" href="#TTCME.pdf.pdfTT.basis">basis</a></code></li>
<li><code><a title="TTCME.pdf.pdfTT.basis_conditioned" href="#TTCME.pdf.pdfTT.basis_conditioned">basis_conditioned</a></code></li>
<li><code><a title="TTCME.pdf.pdfTT.conditioned_variable_names" href="#TTCME.pdf.pdfTT.conditioned_variable_names">conditioned_variable_names</a></code></li>
<li><code><a title="TTCME.pdf.pdfTT.copy" href="#TTCME.pdf.pdfTT.copy">copy</a></code></li>
<li><code><a title="TTCME.pdf.pdfTT.covariance_matrix" href="#TTCME.pdf.pdfTT.covariance_matrix">covariance_matrix</a></code></li>
<li><code><a title="TTCME.pdf.pdfTT.dofs" href="#TTCME.pdf.pdfTT.dofs">dofs</a></code></li>
<li><code><a title="TTCME.pdf.pdfTT.expected_value" href="#TTCME.pdf.pdfTT.expected_value">expected_value</a></code></li>
<li><code><a title="TTCME.pdf.pdfTT.interpoalte" href="#TTCME.pdf.pdfTT.interpoalte">interpoalte</a></code></li>
<li><code><a title="TTCME.pdf.pdfTT.marginal" href="#TTCME.pdf.pdfTT.marginal">marginal</a></code></li>
<li><code><a title="TTCME.pdf.pdfTT.normalize" href="#TTCME.pdf.pdfTT.normalize">normalize</a></code></li>
<li><code><a title="TTCME.pdf.pdfTT.round" href="#TTCME.pdf.pdfTT.round">round</a></code></li>
<li><code><a title="TTCME.pdf.pdfTT.variable_names" href="#TTCME.pdf.pdfTT.variable_names">variable_names</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>